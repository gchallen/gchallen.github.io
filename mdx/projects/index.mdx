---
title: Projects
noDate: true
---

<div className="lead">

**I write code that helps students learn.**

</div>

I create and maintain a variety of software tools to support my courses.
Most of my projects are published on GitHub, and many are publicly deployed and freely available for educators and students to use and extend.
If you'd like to use something for your own courses and need help, please get in touch!
I'm happy to help and, to some degree, what I've made publicly available is largely a result of what people have wanted to use.

## `learncs.online`: Learn to Program Online ((learncsonline))

[`learncs.online`](https://learncs.online) is the freely-available version of the materials developed to support [CS 124](https://www.cs124.org/)—my CS1 course at Illinois.

[`learncs.online`](https://learncs.online) provides public access to many of the same novel features that have helped make CS 124 so successful:

* [61 daily lessons](https://www.learncs.online/lessons) introducing programming and computer science concepts in both Java and Kotlin
* 902 runnable and editable [playground examples](https://www.learncs.online/best#playgrounds)
* 2,540 [interactive walkthroughs](https://www.learncs.online/best#interactive-walkthroughs) from [264 instructors](https://www.learncs.online/people)
* 132 [homework problems](https://www.learncs.online/best#homework) providing both correctness and [code quality feedback](https://www.learncs.online/best#code-quality)
* novel [debugging challenges](https://www.learncs.online/best#debugging-challenges) to train students to fix mistakes

Many of the components supporting [`learncs.online`](https://learncs.online/) emerged from projects described below.

**Demo**: https://learncs.online/  
**Credits**: Hundreds of course staff have contributed content to `learncs.online`, creating the first crowdsourced CS1 course. Find the [full list of contributors here](https://www.learncs.online/people).

## Interactive Code Walkthroughs ((walkthroughs))

One of the most distinctive features of the [`learncs.online`](https://learncs.online) and [CS 124](https://cs124.org) materials is their use of [_interactive walkthroughs_](https://www.learncs.online/best#interactive-walkthroughs).

This novel component combines the best features of recorded live coding—being able to watch exactly what the instructor is doing and listen to their explanation—while still maintaining the interactive features of a playground.
Because the interactive walkthrough is an animated editor and _not_ a video, students can pause at any point and edit or execute the code.

Interactive walkthroughs are also recorded _in-situ_ directly on the page where they are viewed, providing authors with awareness of how that explanation is used within the context of the surrounding information.
By lowering the barrier to providing this kind of content, we have been able to incorporate contributions [hundreds of CS 124 course staff](https://www.learncs.online/people), significantly diversifying the course's voice.

We are working on making this explanatory component more broadly available^[[EdStem Lessons](https://edstem.org/lessons) also has a similar feature in beta, if you'd prefer a commercial alternative.].
Stay tuned!

**Demo**: https://www.learncs.online/best#interactive-walkthroughs  
**Credits**: [Ruisong Li](https://www.ruisong.info/) contributed code to the latest implementation of the interactive walkthrough component.

## Rapid and Accurate Programming Exercise Authoring and Autograding ((questioner))

Students learning to program benefit from practice.
But the traditional approach to writing autograded programming questions by authoring test suite is slow, inaccurate, and ineffective.
Writing test suites that are resistant to memoization and catch all corner cases requires tediously enumerating many inputs.
Test suite authoring provides no feedback to instructors about how effective their test suites are, leaving them unsure about how many test cases are needed.
And software testing frameworks are not designed to be used without access to the tests themselves, which many autograders hide from students during the grading process.

We created a system called Questioner to address these weaknesses.
Questioner leverages a key difference between typical software testing and autograding—when grading, the solution is known!
Instead of requiring instructors write test suites, Questioner has them provide a solution and automates as much of the rest of the process as possible.
A testing strategy is created by identifying the methods and inputs that comprise the solution's public API.
That strategy is validated by ensuring that it can identify incorrect examples, most of which are created automatically by mutating the solution.
As a dedicated autograder, Questioner's output is crafted for beginning programmers.
For example, when testing objects, the entire sequence of calls and inputs that led to the failure is provided, mirroring the information that would normally be gleaned by examining the test suites themselves.

Since Fall 2020 I have used Questioner to author over 700 programming exercises to support my CS1 course, covering all aspects of conceptual thinking taught in the course: from the use of print statements on day 1, to the implementation of recursive sorting algorithms at the end of the semester.
Questioner has resulted in an order-of-magnitude speed up in the time required to author accurate questions, allowing me to provide new questions for each weekly assessment, as well as a large and growing library of practice problems to support student learning.

Questioner supports the publicly-available problems on [`learncs.online`](https://learncs.online/practice).
We are also working to make this tool more broadly available at [`beyondgrader.com`](https://www.beyondgrader.com/).

**Demo**: https://www.learncs.online/best#homework, https://beyondgrader.com  
**Credits**: [Ben Nordick](https://github.com/Fleex255) and [Max Kopinsky](https://www.linkedin.com/in/max-kopinsky-2bbb5719b) worked on an early prototype of the Questioner system.
Several other students, including [Harsh Deep](https://github.com/harsh183) and [Miguel Fernandez](https://www.linkedin.com/in/migujorg), have authored questions using Questioner.

## Autograding Code Quality ((quality))

An oft-cited weakness of autograding is that students don't get feedback on the _quality_ of their code.
Even correct code can have many undesirable features: It may be overly complex, difficult to read, or use too many resources.
However, manual evaluation of student code by human graders is error-prone, incredibly inefficient, and frequently results in feedback delivered far too late to support student learning.

Our goal is to provide students with automated code _quality_ feedback—in real time, on every submission, and with no human input.
This allows students to learn from and correct code quality mistakes, while ensuring that our staff are free to do what they do best: support student success through individual support.

We are using [Questioner](#questioner) to push the boundaries of what is possible with automated code quality analysis.
Currently, through a combination of source and bytecode analysis, we are able to automatically evaluate multiple code quality metrics—including linting (format checking), cyclomatic complexity, runtime and memory efficiency, source line counts, dead code detection, and recursion and feature analysis.

For example, a submission that is correct, but utilizes many more code paths than the solution, usually represents a misunderstanding of the problem and can be simplified.
Dead or unexecuted code again usually points to a misunderstanding of the problem and can be removed.
Feature analysis allows us to do things like prevent students from solving a problem with a loop, or ensure that they submit a recursive solution when required.
Immediate feedback allows students to gradually learn how to craft solutions that are both correct _and_ high-quality.

**Demo**: https://www.learncs.online/best#code-quality, https://beyondgrader.com    
**Credits**: [Ben Nordick](https://github.com/Fleex255) has implemented several of the bytecode-based code quality measures, including code and memory efficiency.
Our code quality toolchain utilizes [Jeed](#jeed) to perform complexity and feature analysis.

## Speedy JVM Execution and Analysis ((jeed))

[Jeed](https://jeed.run) ([GitHub](https://github.com/cs124-illinois/jeed)) is the speedy JVM execution and analysis toolkit.

Jeed's primary purpose is to enable fast and safe execution of untrusted Java and Kotlin code.
It's what powers the Java and Kotlin playgrounds on this site, [`learncs.online`](https://www.learncs.online/best#playgrounds), and the CS 124 website.
It also powers our novel [homework autograder](https://www.learncs.online/best#homework) and [debugging question generation](https://www.learncs.online/best#debugging-challenges).
Jeed runs untrusted code in a secure JVM sandbox 1000 times faster than approaches that rely on containerization.
This performance boost makes it possible to support a large amount of interactive use using a small amount of server resources.
Jeed has been in production use since 2019.

[Find out more about Jeed](https://www.jeed.run/about), or read [about how Jeed was developed](/essays/2022-06-17-the-need-for-jeed).

**Demo**: https://www.learncs.online/best#playgrounds, or https://www.jeed.run/  
**Credits**: [Ben Nordick](https://github.com/Fleex255) has made substantial contributions to Jeed, including critical work on bytecode rewriting required to achieve sandbox safety.
[Ben Clarage](https://www.linkedin.com/in/ben-clarage-856820202/) added initial support for Kotlin source mutation.
[Hania Dziurdzik](https://www.mymozaic.com/hdziurdzik) provided an initial implementation of Java feature analysis.

## Experiments

Projects described below have either not yet been fully completed or are not yet deployed to production in support of one of my courses.

### Polyglot Playgrounds ((polyplayground))

The approach to providing secure high-speed untrusted Java and Kotlin code execution used by [Jeed](#jeed) relies on unique features of the Java Virtual Machine.
A more general is to use containerization to protect the host from untrusted code.
While slower, this allows a single backend to easily support student experimentation with multiple different languages.

**Demo**: https://cs124-illinois.github.io/playground/